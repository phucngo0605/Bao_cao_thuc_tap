# B·∫¢I TO√ÅN 1 
L·ªánh truy c·∫≠p th∆∞ m·ª•c ch·ª©a data
hdfs dfs -ls hdfs://adt-platform-dev-106-254:8120//data/Parquet/AdnLog/2024_12_11/
<img width="1642" height="577" alt="image" src="https://github.com/user-attachments/assets/12e1917d-99b7-408c-ad36-7684077c9baa" />
Ch·∫°y spark-shell 
cd /data/spark-3.4.3/bin/
./spark-shell --master local[*]
<img width="1704" height="436" alt="image" src="https://github.com/user-attachments/assets/27dac31e-18b6-4e1f-a0b0-5835fcb33e4e" />


## 1. M·ª•c ti√™u c·ªßa Code top 10 Domain v·ªõi App
- ƒê·ªçc d·ªØ li·ªáu Parquet t·ª´ th∆∞ m·ª•c `hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewApp/` theo t·ª´ng ng√†y (d·ª±a tr√™n t√™n th∆∞ m·ª•c ki·ªÉu `YYYY_MM_DD`).
- T√≠nh s·ªë l·∫ßn xu·∫•t hi·ªán (`count`) c·ªßa m·ªói `appId` trong m·ªói ng√†y.
- C·ªông d·ªìn k·∫øt qu·∫£ qua c√°c ng√†y ƒë·ªÉ t·∫°o t·ªïng h·ª£p cu·ªëi c√πng.
- Hi·ªÉn th·ªã k·∫øt qu·∫£ t·ª´ng ng√†y v√† t·ªïng h·ª£p cu·ªëi c√πng theo th·ª© t·ª± gi·∫£m d·∫ßn c·ªßa `count`.

## 2. C·∫•u tr√∫c Code
### 2.1. Kh·ªüi t·∫°o Spark Session
```scala
val spark = SparkSession.builder().getOrCreate()
import spark.implicits._
```
- T·∫°o ho·∫∑c l·∫•y phi√™n b·∫£n Spark Session hi·ªán c√≥ ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu.
- Import c√°c h√†m implict ƒë·ªÉ h·ªó tr·ª£ thao t√°c DataFrame.

### 2.2. Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n v√† FileSystem
```scala
val baseDir = "hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewApp/"
val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
```
- ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n c∆° s·ªü (`baseDir`) ƒë·∫øn th∆∞ m·ª•c HDFS ch·ª©a d·ªØ li·ªáu Parquet.
- L·∫•y ƒë·ªëi t∆∞·ª£ng `FileSystem` ƒë·ªÉ li·ªát k√™ c√°c file v√† th∆∞ m·ª•c.

### 2.3. L·∫•y danh s√°ch th∆∞ m·ª•c ng√†y
```scala
val dayDirs = fs.listStatus(new Path(baseDir))
  .map(_.getPath)
  .filter(p => p.getName.matches("\\d{4}_\\d{2}_\\d{2}"))
  .sortBy(_.getName)
```
- L·∫•y danh s√°ch c√°c th∆∞ m·ª•c con trong `baseDir`.
- L·ªçc c√°c th∆∞ m·ª•c c√≥ t√™n kh·ªõp v·ªõi ƒë·ªãnh d·∫°ng ng√†y (`YYYY_MM_DD`).
- S·∫Øp x·∫øp theo th·ª© t·ª± tƒÉng d·∫ßn c·ªßa t√™n ng√†y.

### 2.4. X·ª≠ l√Ω t·ª´ng ng√†y
```scala
var runningTotalDF: DataFrame = spark.emptyDataFrame

for (dayPath <- dayDirs) {
  val day = dayPath.getName
  println(s"\nüü¢ ƒêang x·ª≠ l√Ω ng√†y: $day")

  val parquetFiles = fs.listStatus(dayPath)
    .map(_.getPath.toString)
    .filter(_.endsWith(".parquet"))

  if (parquetFiles.isEmpty) {
    println(s"‚ö†Ô∏è Kh√¥ng c√≥ file parquet trong $day")
  } else {
    val batchSize = 1
    val fileGroups = parquetFiles.grouped(batchSize).toList

    var dayCounts = scala.collection.mutable.Map[String, Long]()

    for ((group, idx) <- fileGroups.zipWithIndex) {
      println(s"   üì¶ Nh√≥m ${idx + 1}/${fileGroups.size}")
      val df = spark.read.parquet(group: _*)
        .select("appId")
        .groupBy("appId")
        .agg(count("*").as("count"))
        .collect()

      df.foreach { row =>
        val appId = row.getString(0)
        val count = row.getLong(1)
        dayCounts(appId) = dayCounts.getOrElse(appId, 0L) + count
      }
    }

    val reducedDayDF = dayCounts.toSeq.toDF("appId", "count")

    println(s"üìä K·∫øt qu·∫£ cho ng√†y $day:")
    reducedDayDF.orderBy(desc("count")).show(truncate = false)

    if (runningTotalDF.isEmpty) {
      runningTotalDF = reducedDayDF
    } else {
      val runningMap = runningTotalDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap
      val newDayMap = reducedDayDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap

      val merged = (runningMap.keySet ++ newDayMap.keySet).map { appId =>
        appId -> (runningMap.getOrElse(appId, 0L) + newDayMap.getOrElse(appId, 0L))
      }.toSeq

      runningTotalDF = merged.toDF("appId", "count")
    }
  }
}
```
- **V√≤ng l·∫∑p qua c√°c ng√†y**: Duy·ªát qua t·ª´ng th∆∞ m·ª•c ng√†y.
- **L·∫•y file Parquet**: Li·ªát k√™ c√°c file `.parquet` trong th∆∞ m·ª•c ng√†y.
- **X·ª≠ l√Ω theo batch**: Chia file th√†nh c√°c nh√≥m (batch size = 1), x·ª≠ l√Ω t·ª´ng nh√≥m ƒë·ªÉ tr√°nh t·∫£i to√†n b·ªô d·ªØ li·ªáu c√πng l√∫c.
- **T√≠nh count**: ƒê·ªçc file, ch·ªçn c·ªôt `appId`, nh√≥m v√† t√≠nh t·ªïng (`count`), l∆∞u v√†o `dayCounts` (Map mutable).
- **T·∫°o DataFrame ng√†y**: Chuy·ªÉn `dayCounts` th√†nh DataFrame (`reducedDayDF`) v√† hi·ªÉn th·ªã.
- **C·ªông d·ªìn**: N·∫øu `runningTotalDF` r·ªóng, g√°n `reducedDayDF`; n·∫øu kh√¥ng, h·ª£p nh·∫•t b·∫±ng c√°ch c·ªông c√°c gi√° tr·ªã `count` c·ªßa `appId` gi·ªëng nhau.

### 2.5. Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng
```scala
if (!runningTotalDF.isEmpty) {
  println("\nüèÅ K·∫øt qu·∫£ t·ªïng h·ª£p to√†n b·ªô:")
  runningTotalDF.orderBy(desc("count")).show(100, truncate = false)
} else {
  println("üö´ Kh√¥ng c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω.")
}
```
- Hi·ªÉn th·ªã DataFrame t·ªïng h·ª£p (`runningTotalDF`) theo th·ª© t·ª± gi·∫£m d·∫ßn c·ªßa `count`, t·ªëi ƒëa 100 d√≤ng.
- N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, in th√¥ng b√°o.

## 3. ∆Øu ƒëi·ªÉm
- **T√≠nh linh ho·∫°t**: X·ª≠ l√Ω t·ª´ng ng√†y v√† batch file, ph√π h·ª£p v·ªõi d·ªØ li·ªáu l·ªõn.
- **Ghi log**: In ti·∫øn tr√¨nh (ng√†y, nh√≥m file) gi√∫p theo d√µi.
- **C·ªông d·ªìn hi·ªáu qu·∫£**: S·ª≠ d·ª•ng Map ƒë·ªÉ h·ª£p nh·∫•t d·ªØ li·ªáu, tr√°nh l·∫∑p t√≠nh to√°n.

## 4. H·∫°n ch·∫ø
- **B·ªô nh·ªõ**: S·ª≠ d·ª•ng `collect()` v√† Map mutable c√≥ th·ªÉ g√¢y treo n·∫øu d·ªØ li·ªáu qu√° l·ªõn, ƒë·∫∑c bi·ªát khi ch·∫°y tr√™n cluster remote.
- **Kh√¥ng t·ªëi ∆∞u Spark**: Kh√¥ng s·ª≠ d·ª•ng lazy evaluation (action `collect()` qu√° s·ªõm), d·∫´n ƒë·∫øn t·∫£i to√†n b·ªô d·ªØ li·ªáu v√†o driver.
- **Batch size c·ªë ƒë·ªãnh**: Batch size = 1 c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u, ph·ª• thu·ªôc v√†o s·ªë l∆∞·ª£ng file.

## 5. ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn
- **S·ª≠ d·ª•ng lazy evaluation**: Thay `collect()` b·∫±ng DataFrame join ho·∫∑c aggregation ƒë·ªÉ gi·∫£m t·∫£i driver.
- **T·ªëi ∆∞u batch size**: ƒê·ªông t√≠nh batch size d·ª±a tr√™n s·ªë file (v√≠ d·ª•: `batchSize = math.max(1, parquetFiles.length / 10)`).
- **Cache c√≥ ch·ªçn l·ªçc**: S·ª≠ d·ª•ng `.persist()` thay v√¨ Map n·∫øu c·∫ßn, v·ªõi m·ª©c ƒë·ªô l∆∞u tr·ªØ th·∫•p.
- **Ch·∫°y tr√™n cluster**: C·∫•u h√¨nh Spark remote (YARN) v·ªõi `--master yarn --deploy-mode client` v√† ƒëi·ªÅu ch·ªânh t√†i nguy√™n.

## 6. K·∫øt qu·∫£ mong ƒë·ª£i
- **M·ªói ng√†y**: Hi·ªÉn th·ªã b·∫£ng `appId` v√† `count` theo th·ª© t·ª± gi·∫£m d·∫ßn:
<img width="499" height="725" alt="image" src="https://github.com/user-attachments/assets/ee6e1bef-78ec-4b54-a2b3-36c39a32380c" />
<img width="464" height="714" alt="image" src="https://github.com/user-attachments/assets/964994fd-b6d8-4269-98a6-7457f77dd0f7" />


- **T·ªïng h·ª£p**: Hi·ªÉn th·ªã b·∫£ng t·ªïng h·ª£p:
<img width="467" height="769" alt="image" src="https://github.com/user-attachments/assets/efd37712-7c86-4c8d-bfd2-57cdb5ba0f8f" />

# Ph√¢n t√≠ch v√† Tr√¨nh b√†y ƒêo·∫°n Code Scala Spark (PageViewMobile)

ƒêo·∫°n code Scala s·ª≠ d·ª•ng Spark ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu Parquet t·ª´ th∆∞ m·ª•c `PageViewMobile` tr√™n HDFS, t·∫≠p trung v√†o vi·ªác t√≠nh t·ªïng s·ªë l∆∞·ª£ng (`count`) c·ªßa c·ªôt `domain` theo t·ª´ng ng√†y v√† c·ªông d·ªìn k·∫øt qu·∫£ qua c√°c ng√†y. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ c·∫•u tr√∫c, ch·ª©c nƒÉng, v√† c√°ch ho·∫°t ƒë·ªông c·ªßa code.

## 1. M·ª•c ti√™u c·ªßa Code
- ƒê·ªçc d·ªØ li·ªáu Parquet t·ª´ th∆∞ m·ª•c `hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewMobile/` theo t·ª´ng ng√†y (d·ª±a tr√™n t√™n th∆∞ m·ª•c ki·ªÉu `YYYY_MM_DD`).
- T√≠nh s·ªë l·∫ßn xu·∫•t hi·ªán (`count`) c·ªßa m·ªói `domain` trong m·ªói ng√†y.
- C·ªông d·ªìn k·∫øt qu·∫£ qua c√°c ng√†y ƒë·ªÉ t·∫°o t·ªïng h·ª£p cu·ªëi c√πng.
- Hi·ªÉn th·ªã k·∫øt qu·∫£ t·ª´ng ng√†y v√† t·ªïng h·ª£p cu·ªëi c√πng theo th·ª© t·ª± gi·∫£m d·∫ßn c·ªßa `count`.

## 2. C·∫•u tr√∫c Code
### 2.1. Kh·ªüi t·∫°o Spark Session
```scala
val spark = SparkSession.builder().getOrCreate()
import spark.implicits._
```
- T·∫°o ho·∫∑c l·∫•y phi√™n b·∫£n Spark Session hi·ªán c√≥ ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu.
- Import c√°c h√†m implict ƒë·ªÉ h·ªó tr·ª£ thao t√°c DataFrame.

### 2.2. Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n v√† FileSystem
```scala
val baseDir = "hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewMobile/"
val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
```
- ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n c∆° s·ªü (`baseDir`) ƒë·∫øn th∆∞ m·ª•c HDFS ch·ª©a d·ªØ li·ªáu Parquet t·ª´ `PageViewMobile`.
- L·∫•y ƒë·ªëi t∆∞·ª£ng `FileSystem` ƒë·ªÉ li·ªát k√™ c√°c file v√† th∆∞ m·ª•c.

### 2.3. L·∫•y danh s√°ch th∆∞ m·ª•c ng√†y
```scala
val dayDirs = fs.listStatus(new Path(baseDir))
  .map(_.getPath)
  .filter(p => p.getName.matches("\\d{4}_\\d{2}_\\d{2}"))
  .sortBy(_.getName)
```
- L·∫•y danh s√°ch c√°c th∆∞ m·ª•c con trong `baseDir`.
- L·ªçc c√°c th∆∞ m·ª•c c√≥ t√™n kh·ªõp v·ªõi ƒë·ªãnh d·∫°ng ng√†y (`YYYY_MM_DD`).
- S·∫Øp x·∫øp theo th·ª© t·ª± tƒÉng d·∫ßn c·ªßa t√™n ng√†y.

### 2.4. X·ª≠ l√Ω t·ª´ng ng√†y
```scala
var runningTotalDF: DataFrame = spark.emptyDataFrame

for (dayPath <- dayDirs) {
  val day = dayPath.getName
  println(s"\nüü¢ ƒêang x·ª≠ l√Ω ng√†y: $day")

  val parquetFiles = fs.listStatus(dayPath)
    .map(_.getPath.toString)
    .filter(_.endsWith(".parquet"))

  if (parquetFiles.isEmpty) {
    println(s"‚ö†Ô∏è Kh√¥ng c√≥ file parquet trong $day")
  } else {
    val batchSize = 1
    val fileGroups = parquetFiles.grouped(batchSize).toList

    var dayCounts = scala.collection.mutable.Map[String, Long]()

    for ((group, idx) <- fileGroups.zipWithIndex) {
      println(s"   üì¶ Nh√≥m ${idx + 1}/${fileGroups.size}")
      val df = spark.read.parquet(group: _*)
        .select("domain")
        .groupBy("domain")
        .agg(count("*").as("count"))
        .collect()

      df.foreach { row =>
        val domain = row.getString(0)
        val count = row.getLong(1)
        dayCounts(domain) = dayCounts.getOrElse(domain, 0L) + count
      }
    }

    val reducedDayDF = dayCounts.toSeq.toDF("domain", "count")

    println(s"üìä K·∫øt qu·∫£ cho ng√†y $day:")
    reducedDayDF.orderBy(desc("count")).show(truncate = false)

    if (runningTotalDF.isEmpty) {
      runningTotalDF = reducedDayDF
    } else {
      val runningMap = runningTotalDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap
      val newDayMap = reducedDayDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap

      val merged = (runningMap.keySet ++ newDayMap.keySet).map { domain =>
        domain -> (runningMap.getOrElse(domain, 0L) + newDayMap.getOrElse(domain, 0L))
      }.toSeq

      runningTotalDF = merged.toDF("domain", "count")
    }
  }
}
```
- **V√≤ng l·∫∑p qua c√°c ng√†y**: Duy·ªát qua t·ª´ng th∆∞ m·ª•c ng√†y.
- **L·∫•y file Parquet**: Li·ªát k√™ c√°c file `.parquet` trong th∆∞ m·ª•c ng√†y.
- **X·ª≠ l√Ω theo batch**: Chia file th√†nh c√°c nh√≥m (batch size = 1), x·ª≠ l√Ω t·ª´ng nh√≥m ƒë·ªÉ tr√°nh t·∫£i to√†n b·ªô d·ªØ li·ªáu c√πng l√∫c.
- **T√≠nh count**: ƒê·ªçc file, ch·ªçn c·ªôt `domain`, nh√≥m v√† t√≠nh t·ªïng (`count`), l∆∞u v√†o `dayCounts` (Map mutable).
- **T·∫°o DataFrame ng√†y**: Chuy·ªÉn `dayCounts` th√†nh DataFrame (`reducedDayDF`) v√† hi·ªÉn th·ªã.
- **C·ªông d·ªìn**: N·∫øu `runningTotalDF` r·ªóng, g√°n `reducedDayDF`; n·∫øu kh√¥ng, h·ª£p nh·∫•t b·∫±ng c√°ch c·ªông c√°c gi√° tr·ªã `count` c·ªßa `domain` gi·ªëng nhau.

### 2.5. Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng
```scala
if (!runningTotalDF.isEmpty) {
  println("\nüèÅ K·∫øt qu·∫£ t·ªïng h·ª£p to√†n b·ªô:")
  runningTotalDF.orderBy(desc("count")).show(10, truncate = false)
} else {
  println("üö´ Kh√¥ng c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω.")
}
```
- Hi·ªÉn th·ªã DataFrame t·ªïng h·ª£p (`runningTotalDF`) theo th·ª© t·ª± gi·∫£m d·∫ßn c·ªßa `count`, t·ªëi ƒëa 10 d√≤ng.
- N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, in th√¥ng b√°o.

## 3. ∆Øu ƒëi·ªÉm
- **T√≠nh linh ho·∫°t**: X·ª≠ l√Ω t·ª´ng ng√†y v√† batch file, ph√π h·ª£p v·ªõi d·ªØ li·ªáu l·ªõn.
- **Ghi log**: In ti·∫øn tr√¨nh (ng√†y, nh√≥m file) gi√∫p theo d√µi.
- **C·ªông d·ªìn hi·ªáu qu·∫£**: S·ª≠ d·ª•ng Map ƒë·ªÉ h·ª£p nh·∫•t d·ªØ li·ªáu, tr√°nh l·∫∑p t√≠nh to√°n.

## 4. H·∫°n ch·∫ø
- **B·ªô nh·ªõ**: S·ª≠ d·ª•ng `collect()` v√† Map mutable c√≥ th·ªÉ g√¢y treo n·∫øu d·ªØ li·ªáu qu√° l·ªõn, ƒë·∫∑c bi·ªát khi ch·∫°y tr√™n cluster remote.
- **Kh√¥ng t·ªëi ∆∞u Spark**: Kh√¥ng s·ª≠ d·ª•ng lazy evaluation (action `collect()` qu√° s·ªõm), d·∫´n ƒë·∫øn t·∫£i to√†n b·ªô d·ªØ li·ªáu v√†o driver.
- **Batch size c·ªë ƒë·ªãnh**: Batch size = 1 c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u, ph·ª• thu·ªôc v√†o s·ªë l∆∞·ª£ng file.

## 5. ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn
- **S·ª≠ d·ª•ng lazy evaluation**: Thay `collect()` b·∫±ng DataFrame join ho·∫∑c aggregation ƒë·ªÉ gi·∫£m t·∫£i driver.
- **T·ªëi ∆∞u batch size**: ƒê·ªông t√≠nh batch size d·ª±a tr√™n s·ªë file (v√≠ d·ª•: `batchSize = math.max(1, parquetFiles.length / 10)`).
- **Cache c√≥ ch·ªçn l·ªçc**: S·ª≠ d·ª•ng `.persist()` thay v√¨ Map n·∫øu c·∫ßn, v·ªõi m·ª©c ƒë·ªô l∆∞u tr·ªØ th·∫•p.
- **Ch·∫°y tr√™n cluster**: C·∫•u h√¨nh Spark remote (YARN) v·ªõi `--master yarn --deploy-mode client` v√† ƒëi·ªÅu ch·ªânh t√†i nguy√™n.

## 6. K·∫øt qu·∫£ mong ƒë·ª£i
- **M·ªói ng√†y**: Hi·ªÉn th·ªã b·∫£ng `domain` v√† `count` theo th·ª© t·ª± gi·∫£m d·∫ßn:
<img width="315" height="543" alt="image" src="https://github.com/user-attachments/assets/c19cada5-f0ca-4b59-a27c-a83bcb78a034" />
<img width="329" height="595" alt="image" src="https://github.com/user-attachments/assets/f43567a3-1f69-4ce2-89a3-93e0282e5f29" />



- **T·ªïng h·ª£p**: Hi·ªÉn th·ªã b·∫£ng t·ªïng h·ª£p:
<img width="401" height="364" alt="image" src="https://github.com/user-attachments/assets/b8c6fc8c-9dbc-409f-93bd-175b433a4096" />



# Ph√¢n t√≠ch v√† Tr√¨nh b√†y ƒêo·∫°n Code Scala Spark (PageViewV1)

ƒêo·∫°n code Scala s·ª≠ d·ª•ng Spark ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu Parquet t·ª´ th∆∞ m·ª•c `PageViewV1` tr√™n HDFS, t·∫≠p trung v√†o vi·ªác t√≠nh t·ªïng s·ªë l∆∞·ª£ng (`count`) c·ªßa c·ªôt `domain` theo t·ª´ng ng√†y v√† c·ªông d·ªìn k·∫øt qu·∫£ qua c√°c ng√†y. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ c·∫•u tr√∫c, ch·ª©c nƒÉng, v√† c√°ch ho·∫°t ƒë·ªông c·ªßa code.

## 1. M·ª•c ti√™u c·ªßa Code
- ƒê·ªçc d·ªØ li·ªáu Parquet t·ª´ th∆∞ m·ª•c `hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewV1/` theo t·ª´ng ng√†y (d·ª±a tr√™n t√™n th∆∞ m·ª•c ki·ªÉu `YYYY_MM_DD`).
- T√≠nh s·ªë l·∫ßn xu·∫•t hi·ªán (`count`) c·ªßa m·ªói `domain` trong m·ªói ng√†y.
- C·ªông d·ªìn k·∫øt qu·∫£ qua c√°c ng√†y ƒë·ªÉ t·∫°o t·ªïng h·ª£p cu·ªëi c√πng.
- Hi·ªÉn th·ªã k·∫øt qu·∫£ t·ª´ng ng√†y v√† t·ªïng h·ª£p cu·ªëi c√πng theo th·ª© t·ª± gi·∫£m d·∫ßn c·ªßa `count`.

## 2. C·∫•u tr√∫c Code
### 2.1. Kh·ªüi t·∫°o Spark Session
```scala
val spark = SparkSession.builder().getOrCreate()
import spark.implicits._
```
- T·∫°o ho·∫∑c l·∫•y phi√™n b·∫£n Spark Session hi·ªán c√≥ ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu.
- Import c√°c h√†m implict ƒë·ªÉ h·ªó tr·ª£ thao t√°c DataFrame.

### 2.2. Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n v√† FileSystem
```scala
val baseDir = "hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewV1/"
val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
```
- ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n c∆° s·ªü (`baseDir`) ƒë·∫øn th∆∞ m·ª•c HDFS ch·ª©a d·ªØ li·ªáu Parquet t·ª´ `PageViewV1`.
- L·∫•y ƒë·ªëi t∆∞·ª£ng `FileSystem` ƒë·ªÉ li·ªát k√™ c√°c file v√† th∆∞ m·ª•c.

### 2.3. L·∫•y danh s√°ch th∆∞ m·ª•c ng√†y
```scala
val dayDirs = fs.listStatus(new Path(baseDir))
  .map(_.getPath)
  .filter(p => p.getName.matches("\\d{4}_\\d{2}_\\d{2}"))
  .sortBy(_.getName)
```
- L·∫•y danh s√°ch c√°c th∆∞ m·ª•c con trong `baseDir`.
- L·ªçc c√°c th∆∞ m·ª•c c√≥ t√™n kh·ªõp v·ªõi ƒë·ªãnh d·∫°ng ng√†y (`YYYY_MM_DD`).
- S·∫Øp x·∫øp theo th·ª© t·ª± tƒÉng d·∫ßn c·ªßa t√™n ng√†y.

### 2.4. X·ª≠ l√Ω t·ª´ng ng√†y
```scala
var runningTotalDF: DataFrame = spark.emptyDataFrame

for (dayPath <- dayDirs) {
  val day = dayPath.getName
  println(s"\nüü¢ ƒêang x·ª≠ l√Ω ng√†y: $day")

  val parquetFiles = fs.listStatus(dayPath)
    .map(_.getPath.toString)
    .filter(_.endsWith(".parquet"))

  if (parquetFiles.isEmpty) {
    println(s"‚ö†Ô∏è Kh√¥ng c√≥ file parquet trong $day")
  } else {
    val batchSize = 1
    val fileGroups = parquetFiles.grouped(batchSize).toList

    var dayCounts = scala.collection.mutable.Map[String, Long]()

    for ((group, idx) <- fileGroups.zipWithIndex) {
      println(s"   üì¶ Nh√≥m ${idx + 1}/${fileGroups.size}")
      val df = spark.read.parquet(group: _*)
        .select("domain")
        .groupBy("domain")
        .agg(count("*").as("count"))
        .collect()

      df.foreach { row =>
        val domain = row.getString(0)
        val count = row.getLong(1)
        dayCounts(domain) = dayCounts.getOrElse(domain, 0L) + count
      }
    }

    val reducedDayDF = dayCounts.toSeq.toDF("domain", "count")

    println(s"üìä K·∫øt qu·∫£ cho ng√†y $day:")
    reducedDayDF.orderBy(desc("count")).show(truncate = false)

    if (runningTotalDF.isEmpty) {
      runningTotalDF = reducedDayDF
    } else {
      val runningMap = runningTotalDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap
      val newDayMap = reducedDayDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap

      val merged = (runningMap.keySet ++ newDayMap.keySet).map { domain =>
        domain -> (runningMap.getOrElse(domain, 0L) + newDayMap.getOrElse(domain, 0L))
      }.toSeq

      runningTotalDF = merged.toDF("domain", "count")
    }
  }
}
```
- **V√≤ng l·∫∑p qua c√°c ng√†y**: Duy·ªát qua t·ª´ng th∆∞ m·ª•c ng√†y.
- **L·∫•y file Parquet**: Li·ªát k√™ c√°c file `.parquet` trong th∆∞ m·ª•c ng√†y.
- **X·ª≠ l√Ω theo batch**: Chia file th√†nh c√°c nh√≥m (batch size = 1), x·ª≠ l√Ω t·ª´ng nh√≥m ƒë·ªÉ tr√°nh t·∫£i to√†n b·ªô d·ªØ li·ªáu c√πng l√∫c.
- **T√≠nh count**: ƒê·ªçc file, ch·ªçn c·ªôt `domain`, nh√≥m v√† t√≠nh t·ªïng (`count`), l∆∞u v√†o `dayCounts` (Map mutable).
- **T·∫°o DataFrame ng√†y**: Chuy·ªÉn `dayCounts` th√†nh DataFrame (`reducedDayDF`) v√† hi·ªÉn th·ªã.
- **C·ªông d·ªìn**: N·∫øu `runningTotalDF` r·ªóng, g√°n `reducedDayDF`; n·∫øu kh√¥ng, h·ª£p nh·∫•t b·∫±ng c√°ch c·ªông c√°c gi√° tr·ªã `count` c·ªßa `domain` gi·ªëng nhau.

### 2.5. Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng
```scala
if (!runningTotalDF.isEmpty) {
  println("\nüèÅ K·∫øt qu·∫£ t·ªïng h·ª£p to√†n b·ªô:")
  runningTotalDF.orderBy(desc("count")).show(10, truncate = false)
} else {
  println("üö´ Kh√¥ng c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω.")
}
```
- Hi·ªÉn th·ªã DataFrame t·ªïng h·ª£p (`runningTotalDF`) theo th·ª© t·ª± gi·∫£m d·∫ßn c·ªßa `count`, t·ªëi ƒëa 10 d√≤ng.
- N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, in th√¥ng b√°o.

## 3. ∆Øu ƒëi·ªÉm
- **T√≠nh linh ho·∫°t**: X·ª≠ l√Ω t·ª´ng ng√†y v√† batch file, ph√π h·ª£p v·ªõi d·ªØ li·ªáu l·ªõn.
- **Ghi log**: In ti·∫øn tr√¨nh (ng√†y, nh√≥m file) gi√∫p theo d√µi.
- **C·ªông d·ªìn hi·ªáu qu·∫£**: S·ª≠ d·ª•ng Map ƒë·ªÉ h·ª£p nh·∫•t d·ªØ li·ªáu, tr√°nh l·∫∑p t√≠nh to√°n.

## 4. H·∫°n ch·∫ø
- **B·ªô nh·ªõ**: S·ª≠ d·ª•ng `collect()` v√† Map mutable c√≥ th·ªÉ g√¢y treo n·∫øu d·ªØ li·ªáu qu√° l·ªõn, ƒë·∫∑c bi·ªát khi ch·∫°y tr√™n cluster remote.
- **Kh√¥ng t·ªëi ∆∞u Spark**: Kh√¥ng s·ª≠ d·ª•ng lazy evaluation (action `collect()` qu√° s·ªõm), d·∫´n ƒë·∫øn t·∫£i to√†n b·ªô d·ªØ li·ªáu v√†o driver.
- **Batch size c·ªë ƒë·ªãnh**: Batch size = 1 c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u, ph·ª• thu·ªôc v√†o s·ªë l∆∞·ª£ng file.

## 5. ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn
- **S·ª≠ d·ª•ng lazy evaluation**: Thay `collect()` b·∫±ng DataFrame join ho·∫∑c aggregation ƒë·ªÉ gi·∫£m t·∫£i driver.
- **T·ªëi ∆∞u batch size**: ƒê·ªông t√≠nh batch size d·ª±a tr√™n s·ªë file (v√≠ d·ª•: `batchSize = math.max(1, parquetFiles.length / 10)`).
- **Cache c√≥ ch·ªçn l·ªçc**: S·ª≠ d·ª•ng `.persist()` thay v√¨ Map n·∫øu c·∫ßn, v·ªõi m·ª©c ƒë·ªô l∆∞u tr·ªØ th·∫•p.
- **Ch·∫°y tr√™n cluster**: C·∫•u h√¨nh Spark remote (YARN) v·ªõi `--master yarn --deploy-mode client` v√† ƒëi·ªÅu ch·ªânh t√†i nguy√™n.

## 6. K·∫øt qu·∫£ mong ƒë·ª£i
- **M·ªói ng√†y**: Hi·ªÉn th·ªã b·∫£ng `domain` v√† `count` theo th·ª© t·ª± gi·∫£m d·∫ßn:
<img width="328" height="535" alt="image" src="https://github.com/user-attachments/assets/6d468f59-26f0-4622-b630-c7a47ee954bc" />
<img width="368" height="664" alt="image" src="https://github.com/user-attachments/assets/e6c5ea0f-7fa9-4da7-b2ab-4c247f697fb9" />


- **T·ªïng h·ª£p**: Hi·ªÉn th·ªã b·∫£ng t·ªïng h·ª£p:
<img width="442" height="363" alt="image" src="https://github.com/user-attachments/assets/70454a47-22a9-4913-be1d-e4aa3f48cfb0" />


















# üöÄ H∆∞·ªõng D·∫´n Tri·ªÉn Khai v√† Tri·ªÉn Khai D·ª± √Ån AdnLog API

## üìÖ Ng√†y v√† Gi·ªù Hi·ªán T·∫°i
- **Ng√†y**: Ch·ªß Nh·∫≠t, 27 th√°ng 7 nƒÉm 2025
- **Th·ªùi gian**: 19:41 PM +07

## üìã T·ªïng Quan B√†i To√°n
**Y√™u c·∫ßu**: X√¢y d·ª±ng server API tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng user view/click cho campaign/banner theo kho·∫£ng th·ªùi gian, v·ªõi th·ªùi gian ph·∫£n h·ªìi < 1 ph√∫t.

**ƒê·∫ßu v√†o**: 
- Log qu·∫£ng c√°o v·ªõi c√°c tr∆∞·ªùng: `guid` (ID ng∆∞·ªùi d√πng), `campaignId`, `bannerId`, `click_or_view` (false=view, true=click), `time_create`.

**ƒê·∫ßu ra**: 
- Endpoint API tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng user unique ƒë√£ view/click campaign/banner trong kho·∫£ng th·ªùi gian ƒë√£ cho.

---

## üèóÔ∏è Ki·∫øn Tr√∫c Gi·∫£i Ph√°p
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Flask API     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Apache Spark   ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Ngu·ªìn D·ªØ Li·ªáu   ‚îÇ
‚îÇ   (REST API)    ‚îÇ    ‚îÇ  (X·ª≠ l√Ω)        ‚îÇ    ‚îÇ (HDFS/M·∫´u)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                       ‚îÇ                       ‚îÇ
        ‚îÇ                       ‚îÇ                       ‚îÇ
    Gunicorn              B·ªô Nh·ªõ Cache            File Parquet
   (S·∫£n xu·∫•t)           (Hi·ªáu su·∫•t)            (L∆∞u tr·ªØ)
```

**Chi·∫øn l∆∞·ª£c ch√≠nh**: Cache to√†n b·ªô d·ªØ li·ªáu v√†o b·ªô nh·ªõ ‚Üí Th·ª±c hi·ªán truy v·∫•n t·ª´ b·ªô nh·ªõ thay v√¨ ƒëƒ©a ‚Üí Th·ªùi gian ph·∫£n h·ªìi < 1 gi√¢y.

---

## üíª Chi Ti·∫øt Tri·ªÉn Khai

### **1. C√°c File C·ªët L√µi c·ªßa ·ª®ng D·ª•ng**

#### **app.py - ·ª®ng d·ª•ng Flask Ch√≠nh**
```python
from flask import Flask, request, jsonify
from datetime import datetime
from spark_session import get_spark
from data_processor import AdnLogProcessor

app = Flask(__name__)
spark = None
processor = None

def init_app():
    """Kh·ªüi t·∫°o Spark session v√† t·∫£i d·ªØ li·ªáu"""
    global spark, processor
    try:
        # B∆∞·ªõc 1: Kh·ªüi t·∫°o Spark session
        spark = get_spark()

        # B∆∞·ªõc 2: Kh·ªüi t·∫°o b·ªô x·ª≠ l√Ω d·ªØ li·ªáu
        processor = AdnLogProcessor(spark)

        # B∆∞·ªõc 3: T·∫£i v√† cache d·ªØ li·ªáu
        if processor.load_and_cache_data():
            return True
        return False
    except Exception as e:
        logger.error(f"L·ªói kh·ªüi t·∫°o: {str(e)}")
        return False

@app.route("/query", methods=["GET"])
def query_user_count():
    """Endpoint ch√≠nh ƒë·ªÉ truy v·∫•n"""
    start_time = datetime.now()

    # L·∫•y tham s·ªë
    id_type = request.args.get("id_type")
    target_id = request.args.get("id")
    mode = request.args.get("mode")
    from_date = request.args.get("from")
    to_date = request.args.get("to")

    # Ki·ªÉm tra tham s·ªë b·∫Øt bu·ªôc
    if not all([id_type, target_id, mode, from_date, to_date]):
        return jsonify({"error": "Thi·∫øu tham s·ªë b·∫Øt bu·ªôc"}), 400

    # Ki·ªÉm tra ƒë·ªãnh d·∫°ng ng√†y
    try:
        datetime.strptime(from_date, '%Y-%m-%d')
        datetime.strptime(to_date, '%Y-%m-%d')
    except ValueError:
        return jsonify({"error": "ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng YYYY-MM-DD"}), 400

    # Th·ª±c hi·ªán truy v·∫•n Spark
    user_count = processor.query_user_count(id_type, target_id, mode, from_date, to_date)

    # T√≠nh th·ªùi gian ph·∫£n h·ªìi
    end_time = datetime.now()
    query_time = (end_time - start_time).total_seconds()

    return jsonify({
        "success": True,
        "data": {
            "id_type": id_type,
            "id": target_id,
            "mode": mode,
            "from_date": from_date,
            "to_date": to_date,
            "user_count": user_count
        },
        "meta": {
            "query_time_seconds": round(query_time, 3),
            "timestamp": end_time.isoformat()
        }
    })

if __name__ == "__main__":
    if init_app():
        app.run(host="0.0.0.0", port=5000, debug=False, threaded=True)
```

#### **spark_session.py - C·∫•u H√¨nh Spark**
```python
from pyspark.sql import SparkSession
import os

def get_spark():
    """T·∫°o SparkSession v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
    mode = os.environ.get('SPARK_MODE', 'remote')

    if mode == 'remote':
        # K·∫øt n·ªëi v·ªõi Spark cluster
        spark = SparkSession.builder \
            .appName("AdnLogAPI-Remote") \
            .master("spark://adt-platform-dev-106-254:7077") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .config("spark.executor.memory", "2g") \
            .config("spark.executor.cores", "2") \
            .getOrCreate()
    elif mode == 'yarn':
        # S·∫£n xu·∫•t v·ªõi YARN
        spark = SparkSession.builder \
            .appName("AdnLogAPI-YARN") \
            .master("yarn") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()
    else:
        # Local mode cho ph√°t tri·ªÉn
        spark = SparkSession.builder \
            .appName("AdnLogAPI-Local") \
            .master("local[*]") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")
    return spark
```

#### **data_processor.py - Logic X·ª≠ L√Ω D·ªØ Li·ªáu**
```python
from pyspark.sql.functions import from_unixtime, col, countDistinct, to_date
from pyspark.sql.types import StructType, StructField, StringType, BooleanType, LongType
import os

class AdnLogProcessor:
    def __init__(self, spark):
        self.spark = spark
        self.df = None

    def create_sample_data(self):
        """T·∫°o d·ªØ li·ªáu m·∫´u cho ph√°t tri·ªÉn"""
        schema = StructType([
            StructField("guid", StringType(), True),
            StructField("campaignId", StringType(), True),
            StructField("bannerId", StringType(), True),
            StructField("click_or_view", BooleanType(), True),
            StructField("time_create", LongType(), True)
        ])

        # D·ªØ li·ªáu m·∫´u v·ªõi c√°c test cases
        sample_data = [
            ("user1", "12345", "banner1", False, 1720137600000),  # 2024-07-05 view
            ("user2", "12345", "banner1", True, 1720137600000),   # 2024-07-05 click
            ("user3", "12345", "banner2", False, 1720051200000),  # 2024-07-04 view
            ("user1", "67890", "banner3", False, 1720051200000),  # 2024-07-04 view
            ("user4", "12345", "banner1", False, 1719964800000),  # 2024-07-03 view
            ("user5", "12345", "banner1", True, 1719964800000),   # 2024-07-03 click
        ]

        df = self.spark.createDataFrame(sample_data, schema)

        # Chuy·ªÉn ƒë·ªïi timestamp
        df_processed = df.withColumn(
            "event_time",
            from_unixtime(col("time_create") / 1000).cast("timestamp")
        ).withColumn(
            "event_date",
            to_date(col("event_time"))
        )

        return df_processed

    def load_and_cache_data(self):
        """T·∫£i v√† cache d·ªØ li·ªáu"""
        try:
            mode = os.environ.get('SPARK_MODE', 'remote')

            if mode in ['remote', 'yarn']:
                # S·∫£n xu·∫•t: ƒê·ªçc t·ª´ HDFS
                df = self.spark.read.parquet("hdfs://adt-platform-dev-106-254:8120/data/Parquet/AdnLog/*")
                df_processed = df.select(
                    "guid", "campaignId", "bannerId", "click_or_view",
                    col("time_group.time_create").alias("time_create")
                ).withColumn(
                    "event_time",
                    from_unixtime(col("time_create") / 1000).cast("timestamp")
                ).withColumn(
                    "event_date",
                    to_date(col("event_time"))
                )
            else:
                # Ph√°t tri·ªÉn: D·ªØ li·ªáu m·∫´u
                df_processed = self.create_sample_data()

            # Cache v√†o b·ªô nh·ªõ ƒë·ªÉ truy v·∫•n nhanh
            self.df = df_processed.cache()

            # K√≠ch ho·∫°t action ƒë·ªÉ cache th·ª±c s·ª±
            count = self.df.count()
            logger.info(f"Cached {count} records successfully")

            return True
        except Exception as e:
            logger.error(f"L·ªói t·∫£i d·ªØ li·ªáu: {str(e)}")
            return False

    def query_user_count(self, id_type, target_id, mode, from_date, to_date):
        """Truy v·∫•n s·ªë l∆∞·ª£ng user unique"""
        try:
            # Ki·ªÉm tra tham s·ªë
            if id_type not in ["campaignId", "bannerId"]:
                raise ValueError("id_type ph·∫£i l√† 'campaignId' ho·∫∑c 'bannerId'")

            if mode not in ["click", "view"]:
                raise ValueError("mode ph·∫£i l√† 'click' ho·∫∑c 'view'")

            # Chuy·ªÉn mode th√†nh boolean (click=true, view=false)
            is_click = (mode == "click")

            # X√¢y d·ª±ng chu·ªói truy v·∫•n hi·ªáu qu·∫£
            result = self.df.filter(col(id_type) == target_id) \
                           .filter(col("click_or_view") == is_click) \
                           .filter(col("event_date").between(from_date, to_date)) \
                           .agg(countDistinct("guid").alias("user_count")) \
                           .collect()

            return int(result[0]["user_count"]) if result else 0

        except Exception as e:
            logger.error(f"L·ªói truy v·∫•n: {str(e)}")
            raise
```

#### **wsgi.py - ƒêi·ªÉm Nh·∫≠p cho S·∫£n Xu·∫•t**
```python
# Giao di·ªán WSGI cho Gunicorn
# Kh·ªüi t·∫°o ·ª©ng d·ª•ng v·ªõi x·ª≠ l√Ω l·ªói
# C·∫•u h√¨nh logging cho s·∫£n xu·∫•t
```

---

### **2. Quy Tr√¨nh Tri·ªÉn Khai**

#### **B∆∞·ªõc 1: Chu·∫©n B·ªã M√¥i Tr∆∞·ªùng Server**

##### **1.1 Y√™u c·∫ßu h·ªá th·ªëng:**
- **H·ªá ƒëi·ªÅu h√†nh**: Linux (Ubuntu/CentOS)
- **Python**: 3.8+
- **Java**: 8+ (cho Spark)
- **Apache Spark**: 3.4.3+
- **B·ªô nh·ªõ**: 2GB+ RAM

##### **1.2 Ki·ªÉm tra m√¥i tr∆∞·ªùng:**
```bash
python3 --version  # Ph·∫£i >= 3.8
java -version      # Ph·∫£i >= 8
free -h           # Ki·ªÉm tra RAM
df -h             # Ki·ªÉm tra dung l∆∞·ª£ng ƒëƒ©a
```

##### **1.3 T·∫°o th∆∞ m·ª•c d·ª± √°n:**
```bash
mkdir -p ~/adnlog-api
cd ~/adnlog-api
pwd
```

##### **1.4 T·∫£i file qua Teleport:**
- Trong giao di·ªán web Teleport, t√¨m n√∫t "Files" ho·∫∑c "Upload".
- T·∫£i l√™n c√°c file:
  - `adnlog-api-20250725_171824.zip`
  - `adnlog-api-complete.zip`

##### **1.5 Gi·∫£i n√©n file:**
```bash
cd ~/adnlog-api
unzip adnlog-api-20250725_171824.zip
unzip adnlog-api-complete.zip
```

##### **1.6 C·∫•p quy·ªÅn th·ª±c thi:**
```bash
chmod +x *.sh
```

---

#### **B∆∞·ªõc 2: C√†i ƒê·∫∑t M√¥i Tr∆∞·ªùng**

##### **2.1 C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng:**
```bash
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYSPARK_PYTHON=python3
```

##### **2.2 Ch·∫°y Spark ·ªü Local Mode**
- T·∫°o file test:
  ```bash
  cat > spark_test_local.py << 'EOF'
  from pyspark.sql import SparkSession

  # T·∫°o Spark session v·ªõi local mode
  spark = SparkSession.builder \
      .appName("ADNLogTest") \
      .master("local[*]") \
      .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
      .getOrCreate()

  print("‚úÖ Spark session created!")
  print(f"Spark version: {spark.version}")
  print(f"Master: {spark.sparkContext.master}")

  df = spark.range(10)
  print(f"Test count: {df.count()}")

  spark.stop()
  print("‚úÖ Test completed!")
  EOF
  ```
- Ch·∫°y test:
  ```bash
  spark-submit --master local[*] spark_test_local.py
  ```

##### **2.3 C·∫•u h√¨nh PYTHONPATH:**
```bash
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
```
- Ki·ªÉm tra PySpark:
  ```bash
  python3 -c "from pyspark.sql import SparkSession; print('PySpark OK')"
  ```

---

#### **B∆∞·ªõc 3: Ch·∫°y ·ª®ng D·ª•ng**

##### **3.1 C√°ch 1: Ch·∫°y th·ªß c√¥ng v·ªõi thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng**
```bash
cd ~/adnlog-api
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export SPARK_MODE=local
python3 app.py
```

##### **3.2 C√°ch 2: T·∫°o script t·ª± ƒë·ªông**
- T·∫°o `run_app.sh`:
  ```bash
  cat > run_app.sh << 'EOF'
  #!/bin/bash

  # Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng
  export SPARK_HOME=/data/spark-3.4.3
  export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
  export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
  export PYSPARK_PYTHON=python3
  export SPARK_MODE=local

  # Ch·∫°y ·ª©ng d·ª•ng
  python3 app.py
  EOF
  ```
- C·∫•p quy·ªÅn v√† ch·∫°y:
  ```bash
  chmod +x run_app.sh
  ./run_app.sh
  ```

##### **3.3 Ch·∫°y v·ªõi `spark-submit` (khuy·∫øn ngh·ªã):**
```bash
spark-submit --master local[*] app.py
```

---

#### **B∆∞·ªõc 4: Ch·∫°y Server v·ªõi Gunicorn**

##### **4.1 T·∫°o script kh·ªüi ƒë·ªông server**
```bash
cd ~/adnlog-api
cat > start_server.sh << 'EOF'
#!/bin/bash
echo "üöÄ Kh·ªüi ƒë·ªông Server AdnLog API..."

# Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export SPARK_MODE=local

echo "üìç Server s·∫Ω s·∫µn s√†ng t·∫°i: http://localhost:5000"
echo "üìñ T√†i li·ªáu API: http://localhost:5000/"
echo "‚ù§Ô∏è Ki·ªÉm tra s·ª©c kh·ªèe: http://localhost:5000/health"
echo ""

# Kh·ªüi ƒë·ªông server
gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 wsgi:app
EOF
```

##### **4.2 T·∫°o script ki·ªÉm tra API**
```bash
cat > test_api.sh << 'EOF'
#!/bin/bash
echo "üß™ Ki·ªÉm tra AdnLog API..."

echo "1. Ki·ªÉm tra s·ª©c kh·ªèe:"
curl -s http://localhost:5000/health | python3 -m json.tool

echo -e "\n2. T√†i li·ªáu API:"
curl -s http://localhost:5000/ | python3 -m json.tool

echo -e "\n3. Ki·ªÉm tra truy v·∫•n Campaign View:"
curl -s "http://localhost:5000/query?id_type=campaignId&id=12345&mode=view&from=2024-07-03&to=2024-07-05" | python3 -m json.tool

echo -e "\n4. Ki·ªÉm tra truy v·∫•n Campaign Click:"
curl -s "http://localhost:5000/query?id_type=campaignId&id=12345&mode=click&from=2024-07-03&to=2024-07-05" | python3 -m json.tool

echo -e "\n‚úÖ Ho√†n t·∫•t t·∫•t c·∫£ c√°c ki·ªÉm tra!"
EOF
```

##### **4.3 C·∫•p quy·ªÅn v√† ch·∫°y**
```bash
chmod +x start_server.sh test_api.sh
echo "‚úÖ Scripts ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!"
./start_server.sh
```

##### **4.4 Thay ƒë·ªïi c·ªïng (t√πy ch·ªçn)**
- Thay ƒë·ªïi c·ªïng trong `app.py`:
  ```bash
  sed -i 's/port=5000/port=8080/' app.py
  python3 app.py
  ```
- Ho·∫∑c d√πng l·ªánh d·ª± ph√≤ng:
  ```bash
  gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 wsgi:app
  ```

##### **4.5 Ki·ªÉm tra API**
- M·ªü terminal m·ªõi v√† ch·∫°y:
  ```bash
  # Ki·ªÉm tra s·ª©c kh·ªèe
  curl http://localhost:5000/health

  # V√≠ d·ª• truy v·∫•n
  curl "http://localhost:5000/query?id_type=campaignId&id=12345&mode=view&from=2024-07-03&to=2024-07-05"
  ```

---

#### **B∆∞·ªõc 5: Tri·ªÉn Khai T·ª± ƒê·ªông**
```bash
# T·∫£i adnlog-api-complete.zip l√™n server
unzip adnlog-api-complete.zip
cd adnlog-api-complete/
chmod +x *.sh
./setup_environment.sh
./start_server.sh
```

---

## ‚úÖ X√°c Th·ª±c Ch·ª©c NƒÉng
1. **Ch·ª©c nƒÉng c·ªët l√µi**:
   - ‚úì Tr·∫£ v·ªÅ s·ªë user view/click: `"user_count": 3`
   - ‚úì H·ªó tr·ª£ campaign: `"id_type": "campaignId", "id": "12345"`
   - ‚úì H·ªó tr·ª£ banner: API c√≥ endpoint cho `bannerId`
   - ‚úì L·ªçc theo th·ªùi gian: `"from_date": "2024-07-03", "to_date": "2024-07-05"`
   - ‚úì Ph√¢n bi·ªát view/click: `"mode": "view"` (false = view, true = click)

2. **Hi·ªáu su·∫•t**:
   - Y√™u c·∫ßu: < 1 ph√∫t
   - Th·ª±c t·∫ø: `"query_time_seconds": 0.929` (< 1 gi√¢y!)
   - K·∫øt qu·∫£: Nhanh h∆°n y√™u c·∫ßu 60 l·∫ßn! üöÄ

3. **C·∫•u tr√∫c d·ªØ li·ªáu**:
   - Server hi·ªÉu ƒë√∫ng c·∫•u tr√∫c log:
     - `guid` ‚Üí ƒê·ªãnh danh user ‚úì
     - `campaignId` ‚Üí ID chi·∫øn d·ªãch ‚úì
     - `bannerId` ‚Üí ID banner ‚úì
     - `click_or_view` ‚Üí false=view, true=click ‚úì

4. **Thi·∫øt k·∫ø API**:
   - RESTful: Endpoint GET v·ªõi tham s·ªë truy v·∫•n
   - Linh ho·∫°t: H·ªó tr·ª£ c·∫£ `campaignId` v√† `bannerId`
   - R√µ r√†ng: ƒê·ªãnh d·∫°ng ph·∫£n h·ªìi r√µ r√†ng v·ªõi metadata
   - X·ª≠ l√Ω l·ªói: Ki·ªÉm tra tham s·ªë ƒë·∫ßu v√†o

#### **Tr∆∞·ªùng h·ª£p ki·ªÉm tra**
```bash
# Ki·ªÉm tra truy v·∫•n banner
curl "http://localhost:5000/query?id_type=bannerId&id=banner1&mode=click&from=2024-07-03&to=2024-07-05"

# Ki·ªÉm tra tr∆∞·ªùng h·ª£p bi√™n
curl "http://localhost:5000/query?id_type=campaignId&id=99999&mode=view&from=2024-07-01&to=2024-07-02"
```

---

## üîß G·ª° L·ªói v√† T√≠ch H·ª£p HDFS

### **1. Chuy·ªÉn sang Remote/YARN Mode**
#### **T√πy ch·ªçn A: Remote Mode**
```bash
export SPARK_MODE=remote
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
```

### **2. Ki·ªÉm tra D·ªØ li·ªáu HDFS**
```bash
# Ki·ªÉm tra kh·∫£ nƒÉng truy c·∫≠p HDFS
hdfs dfs -ls /data/Parquet/AdnLog/

# Xem c·∫•u tr√∫c th∆∞ m·ª•c
hdfs dfs -ls -R /data/Parquet/AdnLog/ | head -20

# Ki·ªÉm tra dung l∆∞·ª£ng file
hdfs dfs -du -h /data/Parquet/AdnLog/

# Ki·ªÉm tra ƒë·ªçc file Parquet
hdfs dfs -cat /data/Parquet/AdnLog/part-00000.parquet | head -10
```

### **3. Ki·ªÉm tra C·∫•u h√¨nh Hadoop**
```bash
# Ki·ªÉm tra file c·∫•u h√¨nh Hadoop
ls -la $HADOOP_HOME/etc/hadoop/
cat $HADOOP_HOME/etc/hadoop/core-site.xml
cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml

# Ho·∫∑c t√¨m file c·∫•u h√¨nh
find /etc -name "core-site.xml" 2>/dev/null
find /opt -name "core-site.xml" 2>/dev/null
find /data -name "core-site.xml" 2>/dev/null
```

### **4. S·ª≠a C·∫•u h√¨nh HDFS**
```bash
# Thi·∫øt l·∫≠p c·∫•u h√¨nh Hadoop qua bi·∫øn m√¥i tr∆∞·ªùng
export HADOOP_CONF_DIR=/data/hadoop-3.3.5/etc/hadoop
export HDFS_NAMENODE_USER=hdfs1
export HDFS_DATANODE_USER=hdfs1
export HADOOP_OPTS="-Dfs.defaultFS=hdfs://adt-platform-dev-106-254:8120"
```

### **5. Ki·ªÉm tra Spark v·ªõi HDFS**
- T·∫°o script test:
  ```bash
  cat > test_spark_hdfs.py << 'EOF'
  #!/usr/bin/env python3
  import os
  from pyspark.sql import SparkSession

  try:
      # T·∫°o Spark session v·ªõi c·∫•u h√¨nh HDFS override
      spark = SparkSession.builder \
          .appName("HDFS-Direct-Test") \
          .master("local[*]") \
          .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
          .config("spark.hadoop.dfs.nameservices", "") \
          .config("spark.hadoop.dfs.client.failover.proxy.provider", "") \
          .getOrCreate()
      
      print("‚úÖ Spark session created!")
      
      # Test c√°c ƒë∆∞·ªùng d·∫´n HDFS
      hdfs_paths = [
          "hdfs://adt-platform-dev-106-254:8120/data/Parquet/AdnLog/*",
          "hdfs://10.3.106.254:8120/data/Parquet/AdnLog/*",
          "/data/Parquet/AdnLog/*"  # ƒê∆∞·ªùng d·∫´n c·ª•c b·ªô n·∫øu d·ªØ li·ªáu ƒë∆∞·ª£c g·∫Øn
      ]
      
      for path in hdfs_paths:
          try:
              print(f"\nüîç Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n: {path}")
              df = spark.read.parquet(path)
              count = df.count()
              print(f"‚úÖ Th√†nh c√¥ng! T√¨m th·∫•y {count} b·∫£n ghi")
              
              # Hi·ªÉn th·ªã schema
              print("üìã Schema:")
              df.printSchema()
              
              # Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu
              print("üìÑ D·ªØ li·ªáu m·∫´u:")
              df.show(3)
              break
              
          except Exception as e:
              print(f"‚ùå Th·∫•t b·∫°i: {str(e)}")
              continue
      
      spark.stop()
      
  except Exception as e:
      print(f"‚ùå L·ªói Spark session: {str(e)}")
  EOF
  ```
- Ch·∫°y test:
  ```bash
  python3 test_spark_hdfs.py
  ```

---

### **6. Chuy·ªÉn sang D·ªØ li·ªáu HDFS**

#### **6.1 T·∫°o ·ª®ng D·ª•ng Hybrid**
- T·∫°o `app_hybrid.py`:
  ```bash
  cat > app_hybrid.py << 'EOF'
  from flask import Flask, request, jsonify
  from datetime import datetime
  from spark_session import get_spark
  from data_processor import AdnLogProcessor
  import logging

  # C·∫•u h√¨nh logging
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)

  app = Flask(__name__)
  spark = None
  processor = None

  def init_app():
      """Kh·ªüi t·∫°o Spark session v√† t·∫£i d·ªØ li·ªáu"""
      global spark, processor
      try:
          logger.info("=== B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o Server AdnLog ===")
          logger.info("B∆∞·ªõc 1: Kh·ªüi t·∫°o Spark session...")
          
          spark = get_spark()
          logger.info("‚úì Spark session ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng")
          
          logger.info("B∆∞·ªõc 2: Kh·ªüi t·∫°o b·ªô x·ª≠ l√Ω d·ªØ li·ªáu...")
          processor = AdnLogProcessor(spark)
          logger.info("‚úì B·ªô x·ª≠ l√Ω d·ªØ li·ªáu ƒë∆∞·ª£c kh·ªüi t·∫°o")
          
          logger.info("B∆∞·ªõc 3: T·∫£i v√† cache d·ªØ li·ªáu...")
          if processor.load_and_cache_data():
              logger.info("‚úì D·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i v√† cache th√†nh c√¥ng")
              logger.info("=== Ho√†n t·∫•t kh·ªüi t·∫°o server ===")
              return True
          else:
              logger.error("‚úó Th·∫•t b·∫°i khi t·∫£i d·ªØ li·ªáu")
              return False
              
      except Exception as e:
          logger.error(f"L·ªói kh·ªüi t·∫°o: {str(e)}")
          return False

  @app.route("/", methods=["GET"])
  def api_documentation():
      """Endpoint t√†i li·ªáu API"""
      return jsonify({
          "service": "AdnLog Query API",
          "version": "1.0.0",
          "endpoints": {
              "/health": "Ki·ªÉm tra s·ª©c kh·ªèe",
              "/query": "Truy v·∫•n s·ªë l∆∞·ª£ng user cho campaign/banner"
          },
          "query_parameters": {
              "id_type": "campaignId ho·∫∑c bannerId",
              "id": "Gi√° tr·ªã ID m·ª•c ti√™u", 
              "mode": "click ho·∫∑c view",
              "from": "Ng√†y b·∫Øt ƒë·∫ßu (YYYY-MM-DD)",
              "to": "Ng√†y k·∫øt th√∫c (YYYY-MM-DD)"
          },
          "example": "/query?id_type=campaignId&id=12345&mode=view&from=2024-07-01&to=2024-07-05"
      })

  @app.route("/health", methods=["GET"])
  def health_check():
      """Endpoint ki·ªÉm tra s·ª©c kh·ªèe"""
      return jsonify({
          "service": "AdnLog Query API",
          "status": "healthy",
          "spark_status": "active" if spark else "inactive",
          "timestamp": datetime.now().isoformat()
      })

  @app.route("/query", methods=["GET"])
  def query_user_count():
      """Endpoint ch√≠nh ƒë·ªÉ truy v·∫•n"""
      start_time = datetime.now()
      
      try:
          # L·∫•y tham s·ªë
          id_type = request.args.get("id_type")
          target_id = request.args.get("id")
          mode = request.args.get("mode")
          from_date = request.args.get("from")
          to_date = request.args.get("to")
          
          logger.info(f"Nh·∫≠n truy v·∫•n: id_type={id_type}, id={target_id}, mode={mode}, from={from_date}, to={to_date}")
          
          # Ki·ªÉm tra tham s·ªë b·∫Øt bu·ªôc
          if not all([id_type, target_id, mode, from_date, to_date]):
              missing = [param for param, value in [
                  ("id_type", id_type), ("id", target_id), ("mode", mode),
                  ("from", from_date), ("to", to_date)
              ] if not value]
              return jsonify({
                  "success": False,
                  "error": f"Thi·∫øu tham s·ªë b·∫Øt bu·ªôc: {', '.join(missing)}",
                  "meta": {
                      "query_time_seconds": 0.001,
                      "timestamp": datetime.now().isoformat()
                  }
              }), 400
          
          # Ki·ªÉm tra id_type
          if id_type not in ["campaignId", "bannerId"]:
              return jsonify({
                  "success": False,
                  "error": "id_type ph·∫£i l√† 'campaignId' ho·∫∑c 'bannerId'",
                  "meta": {
                      "query_time_seconds": 0.001,
                      "timestamp": datetime.now().isoformat()
                  }
              }), 400
          
          # Ki·ªÉm tra mode
          if mode not in ["click", "view"]:
              return jsonify({
                  "success": False,
                  "error": "mode ph·∫£i l√† 'click' ho·∫∑c 'view'",
                  "meta": {
                      "query_time_seconds": 0.001,
                      "timestamp": datetime.now().isoformat()
                  }
              }), 400
          
          # Ki·ªÉm tra ƒë·ªãnh d·∫°ng ng√†y
          try:
              datetime.strptime(from_date, '%Y-%m-%d')
              datetime.strptime(to_date, '%Y-%m-%d')
          except ValueError:
              return jsonify({
                  "success": False,
                  "error": "ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng YYYY-MM-DD",
                  "meta": {
                      "query_time_seconds": 0.001,
                      "timestamp": datetime.now().isoformat()
                  }
              }), 400
          
          # Th·ª±c hi·ªán truy v·∫•n Spark
          user_count = processor.query_user_count(id_type, target_id, mode, from_date, to_date)
          
          # T√≠nh th·ªùi gian ph·∫£n h·ªìi
          end_time = datetime.now()
          query_time = (end_time - start_time).total_seconds()
          
          logger.info(f"Truy v·∫•n ho√†n t·∫•t trong {query_time:.3f}s, k·∫øt qu·∫£: {user_count} users")
          
          return jsonify({
              "success": True,
              "data": {
                  "id_type": id_type,
                  "id": target_id,
                  "mode": mode,
                  "from_date": from_date,
                  "to_date": to_date,
                  "user_count": user_count
              },
              "meta": {
                  "query_time_seconds": round(query_time, 3),
                  "timestamp": end_time.isoformat()
              }
          })
          
      except Exception as e:
          end_time = datetime.now()
          query_time = (end_time - start_time).total_seconds()
          logger.error(f"L·ªói truy v·∫•n: {str(e)}")
          
          return jsonify({
              "success": False,
              "error": str(e),
              "meta": {
                  "query_time_seconds": round(query_time, 3),
                  "timestamp": end_time.isoformat()
              }
          }), 500

  if __name__ == "__main__":
      print("üöÄ Kh·ªüi ƒë·ªông Server Truy V·∫•n AdnLog...")
      if init_app():
          print("‚úÖ Kh·ªüi t·∫°o th√†nh c√¥ng!")
          print("üåê Server kh·ªüi ƒë·ªông t·∫°i http://0.0.0.0:8080")
          print("üìñ T√†i li·ªáu API: http://localhost:8080/")
          print("‚ù§Ô∏è Ki·ªÉm tra s·ª©c kh·ªèe: http://localhost:8080/health")
          app.run(host="0.0.0.0", port=8080, debug=False, threaded=True)
      else:
          print("‚ùå Kh·ªüi t·∫°o th·∫•t b·∫°i!")
          exit(1)
  EOF
  ```

#### **6.2 Ch·∫°y v·ªõi d·ªØ li·ªáu m·∫´u tr∆∞·ªõc**
```bash
export SPARK_MODE=local
python3 app_hybrid.py
```

#### **6.3 Chuy·ªÉn sang Remote Mode v·ªõi HDFS**
```bash
export SPARK_MODE=remote
python3 app_hybrid.py
```

#### **6.4 T·∫°o script s·∫£n xu·∫•t**
```bash
cat > start_production_hdfs.sh << 'EOF'
#!/bin/bash
echo "üöÄ Kh·ªüi ƒë·ªông AdnLog API v·ªõi d·ªØ li·ªáu HDFS..."

# Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng
export SPARK_MODE=remote
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export HADOOP_CONF_DIR=/data/hadoop-3.3.5/etc/hadoop

# Kh·ªüi ƒë·ªông v·ªõi Gunicorn
gunicorn --bind 0.0.0.0:8080 --workers 2 --timeout 300 --preload app_hybrid:app
EOF
chmod +x start_production_hdfs.sh
```

#### **6.5 C·∫≠p nh·∫≠t `spark_session.py`**
- Sao l∆∞u file g·ªëc:
  ```bash
  cp spark_session.py spark_session.py.backup
  ```
- T·∫°o phi√™n b·∫£n m·ªõi:
  ```bash
  cat > spark_session.py << 'EOF'
  from pyspark.sql import SparkSession
  import os

  def get_spark():
      """T·∫°o SparkSession v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
      mode = os.environ.get('SPARK_MODE', 'remote')

      if mode == 'remote':
          # D√πng local mode nh∆∞ng v·ªõi truy c·∫≠p HDFS
          spark = SparkSession.builder \
              .appName("AdnLogAPI-Remote") \
              .master("local[*]") \
              .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
              .config("spark.sql.adaptive.enabled", "true") \
              .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
              .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
              .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
              .getOrCreate()
      elif mode == 'yarn':
          # S·∫£n xu·∫•t v·ªõi YARN
          spark = SparkSession.builder \
              .appName("AdnLogAPI-YARN") \
              .master("yarn") \
              .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
              .config("spark.sql.adaptive.enabled", "true") \
              .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
              .getOrCreate()
      else:
          # Local mode cho ph√°t tri·ªÉn
          spark = SparkSession.builder \
              .appName("AdnLogAPI-Local") \
              .master("local[*]") \
              .config("spark.sql.adaptive.enabled", "true") \
              .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
              .getOrCreate()

      spark.sparkContext.setLogLevel("WARN")
      return spark
  EOF
  ```

#### **6.6 Ch·∫°y v·ªõi HDFS**
```bash
export SPARK_MODE=remote
python3 app_hybrid.py
```

---

## ‚ö° Chi·∫øn L∆∞·ª£c T·ªëi ∆Øu Hi·ªáu Su·∫•t

### **1. Cache B·ªô Nh·ªõ**
```python
# D·ªØ li·ªáu ƒë∆∞·ª£c cache v√†o b·ªô nh·ªõ l·∫ßn ƒë·∫ßu
self.df = df_processed.cache()
count = self.df.count()  # K√≠ch ho·∫°t caching

# C√°c truy v·∫•n sau ƒë√°nh v√†o b·ªô nh·ªõ, kh√¥ng ƒëƒ©a
result = self.df.filter(...).agg(countDistinct("guid"))
```

### **2. T·ªëi ∆Øu Spark**
```python
# Adaptive Query Execution
"spark.sql.adaptive.enabled": "true"

# G·ªôp c√°c ph√¢n v√πng nh·ªè  
"spark.sql.adaptive.coalescePartitions.enabled": "true"

# Chu·∫©n h√≥a nhanh
"spark.serializer": "org.apache.spark.serializer.KryoSerializer"

# Chuy·ªÉn d·ªØ li·ªáu d·ª±a tr√™n Arrow
"spark.sql.execution.arrow.pyspark.enabled": "true"
```

### **3. T·ªëi ∆Øu Truy V·∫•n**
```python
# Chu·ªói truy v·∫•n hi·ªáu qu·∫£:
result = self.df.filter(col(id_type) == target_id) \
               .filter(col("click_or_view") == is_click) \
               .filter(col("event_date").between(from_date, to_date)) \
               .agg(countDistinct("guid").alias("user_count"))
```

---

## üîÑ Quy Tr√¨nh X·ª≠ L√Ω Y√™u C·∫ßu

### **S∆° ƒë·ªì Chi Ti·∫øt:**
```
Y√™u c·∫ßu t·ª´ Client
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ·ª®ng d·ª•ng Flask                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ Ki·ªÉm tra        ‚îÇ    ‚îÇ Ki·ªÉm tra ƒë·ªãnh   ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ Tham s·ªë         ‚îÇ ‚Üí  ‚îÇ d·∫°ng ng√†y       ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ (id_type, id,   ‚îÇ    ‚îÇ (YYYY-MM-DD)    ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  mode, dates)   ‚îÇ    ‚îÇ                 ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ           ‚Üì                       ‚Üì                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ              Th·ª±c thi truy v·∫•n Spark                   ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   L·ªçc       ‚îÇ‚Üí ‚îÇ   L·ªçc       ‚îÇ‚Üí ‚îÇ   L·ªçc       ‚îÇ     ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   theo ID   ‚îÇ  ‚îÇ  theo Mode  ‚îÇ  ‚îÇ theo kho·∫£ng ‚îÇ     ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ   th·ªùi gian  ‚îÇ     ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ‚îÇ
‚îÇ  ‚îÇ           ‚Üì              ‚Üì              ‚Üì               ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ        countDistinct("guid")                        ‚îÇ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ        (ƒê·∫øm user unique)                            ‚îÇ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ           ‚Üì                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ              ƒê·ªãnh d·∫°ng ph·∫£n h·ªìi                        ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Th√™m th·ªùi gian th·ª±c thi truy v·∫•n                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Th√™m timestamp                                      ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ƒê·ªãnh d·∫°ng JSON ph·∫£n h·ªìi                             ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
Ph·∫£n h·ªìi JSON ƒë·∫øn Client
```

### **Chi·∫øn l∆∞·ª£c Cache B·ªô Nh·ªõ:**
```
Giai ƒëo·∫°n Kh·ªüi ƒë·ªông:
D·ªØ li·ªáu HDFS/M·∫´u ‚Üí Spark DataFrame ‚Üí .cache() ‚Üí L∆∞u tr·ªØ b·ªô nh·ªõ

Giai ƒëo·∫°n Truy v·∫•n:
Cache b·ªô nh·ªõ ‚Üí Thao t√°c l·ªçc ‚Üí T·ªïng h·ª£p ‚Üí K·∫øt qu·∫£
```

**Th·ªùi gian hi·ªáu su·∫•t**:
- **Kh·ªüi ƒë·ªông**: ~10-15 gi√¢y (t·∫£i + cache d·ªØ li·ªáu)
- **Truy v·∫•n ƒë·∫ßu ti√™n**: < 1 gi√¢y (t·ª´ cache b·ªô nh·ªõ)
- **C√°c truy v·∫•n sau**: < 0.5 gi√¢y (truy c·∫≠p cache t·ªëi ∆∞u)
- **ƒê·ªìng th·ªùi**: 50+ user v·ªõi 2 workers

---

## üêõ X·ª≠ L√Ω C√°c V·∫•n ƒê·ªÅ Th∆∞·ªùng G·∫∑p

### **1. L·ªói Import PySpark**
**Tri·ªáu ch·ª©ng:**
```
ModuleNotFoundError: No module named 'pyspark'
```

**Nguy√™n nh√¢n:** PYTHONPATH kh√¥ng ƒë∆∞·ª£c thi·∫øt l·∫≠p ƒë√∫ng

**Gi·∫£i ph√°p:**
```bash
# Ki·ªÉm tra SPARK_HOME
echo $SPARK_HOME

# Thi·∫øt l·∫≠p PYTHONPATH th·ªß c√¥ng
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH

# Ki·ªÉm tra import
python3 -c "from pyspark.sql import SparkSession; print('PySpark OK')"

# N·∫øu v·∫´n l·ªói, t√¨m phi√™n b·∫£n py4j ƒë√∫ng
ls -la $SPARK_HOME/python/lib/py4j*.zip
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip:$PYTHONPATH
```

### **2. C·ªïng ƒê√£ ƒê∆∞·ª£c S·ª≠ D·ª•ng**
**Tri·ªáu ch·ª©ng:**
```
Address already in use
Port 5000 is in use by another program
```

**Gi·∫£i ph√°p (kh√¥ng c·∫ßn sudo):**
```bash
# T√¨m ti·∫øn tr√¨nh ƒëang d√πng c·ªïng (ch·ªâ ti·∫øn tr√¨nh c·ªßa user)
lsof -i :5000

# K·∫øt th√∫c ti·∫øn tr√¨nh n·∫øu l√† c·ªßa user
kill -9 <PID>

# Ho·∫∑c d√πng c·ªïng kh√°c (khuy·∫øn ngh·ªã)
sed -i 's/port=5000/port=8888/' app.py
python3 app.py

# Server s·∫Ω ch·∫°y tr√™n http://localhost:8888

# Ho·∫∑c ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng PORT
export PORT=8888
python3 app.py
```

### **3. Spark Kh√¥ng ƒê∆∞·ª£c T√¨m Th·∫•y**
**Tri·ªáu ch·ª©ng:**
```
SPARK_HOME not set or Spark not found
```

**Gi·∫£i ph√°p:**
```bash
# T√¨m c√†i ƒë·∫∑t Spark
find /opt /usr/local /data -name "spark-submit" 2>/dev/null

# Thi·∫øt l·∫≠p SPARK_HOME
export SPARK_HOME="/data/spark-3.4.3"  # Thay ƒë·ªïi path ph√π h·ª£p
echo 'export SPARK_HOME="/data/spark-3.4.3"' >> .env

# Ki·ªÉm tra Spark
$SPARK_HOME/bin/spark-submit --version
```

### **4. V·∫•n ƒê·ªÅ B·ªô Nh·ªõ**
**Tri·ªáu ch·ª©ng:**
```
Java heap space error
OutOfMemoryError
```

**Gi·∫£i ph√°p:**
```bash
# Gi·∫£m dung l∆∞·ª£ng b·ªô nh·ªõ Spark
export SPARK_DRIVER_MEMORY=1g
export SPARK_EXECUTOR_MEMORY=1g

# Ho·∫∑c d√πng √≠t workers
gunicorn --workers 1 wsgi:app

# Ki·ªÉm tra b·ªô nh·ªõ h·ªá th·ªëng
free -h
```

### **5. Server T·ª± D·ª´ng**
**Tri·ªáu ch·ª©ng:** Server kh·ªüi ƒë·ªông nh∆∞ng t·ª± tho√°t ngay

**C√°c b∆∞·ªõc g·ª° l·ªói:**
```bash
# Ch·∫°y mode ph√°t tri·ªÉn ƒë·ªÉ xem l·ªói
./start_dev.sh

# Ho·∫∑c ch·∫°y tr·ª±c ti·∫øp
source .env
python3 app.py

# Ki·ªÉm tra log
tail -f logs/error.log

# Ki·ªÉm tra t·ª´ng th√†nh ph·∫ßn
python3 -c "from pyspark.sql import SparkSession; print('PySpark OK')"
python3 -c "import flask; print('Flask OK')"
python3 -c "from wsgi import application; print('WSGI OK')"
```

### **6. Hi·ªáu Su·∫•t Truy V·∫•n Ch·∫≠m**
**Tri·ªáu ch·ª©ng:** Th·ªùi gian truy v·∫•n > 5 gi√¢y

**T·ªëi ∆∞u h√≥a:**
```bash
# Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c cache ch∆∞a
# Trong log ph·∫£i th·∫•y: "Cached X records successfully"

# TƒÉng ƒë·ªô song song Spark
export SPARK_SQL_ADAPTIVE_COALESCE_PARTITIONS_ENABLED=true

# Theo d√µi Spark UI
# http://localhost:4040 (khi server ƒëang ch·∫°y)
```

### **7. L·ªói K·∫øt N·ªëi HDFS**
**Tri·ªáu ch·ª©ng:**
```
Connection refused: hdfs://server:8120
```

**Gi·∫£i ph√°p:**
```bash
# Chuy·ªÉn sang local mode
export SPARK_MODE=local
python3 app.py

# Ho·∫∑c ki·ªÉm tra k·∫øt n·ªëi HDFS
hdfs dfs -ls /data/Parquet/AdnLog/
```

---

## üìä T√†i Li·ªáu Endpoint API

### **GET /health**
```json
{
  "status": "healthy",
  "service": "AdnLog Query API", 
  "spark_status": "active",
  "timestamp": "2025-07-27T15:53:46.481000"
}
```

### **GET /query**
**Tham s·ªë:**
- `id_type`: `campaignId` ho·∫∑c `bannerId`
- `id`: Gi√° tr·ªã ID m·ª•c ti√™u
- `mode`: `click` ho·∫∑c `view`
- `from`: Ng√†y b·∫Øt ƒë·∫ßu (YYYY-MM-DD)
- `to`: Ng√†y k·∫øt th√∫c (YYYY-MM-DD)

**Ph·∫£n h·ªìi:**
```json
{
  "success": true,
  "data": {
    "user_count": 3,
    "id_type": "campaignId",
    "id": "12345",
    "mode": "view",
    "from_date": "2024-07-03",
    "to_date": "2024-07-05"
  },
  "meta": {
    "query_time_seconds": 0.309,
    "timestamp": "2025-07-27T15:53:46.481000"
  }
}
```

---

## üéØ K·∫øt Qu·∫£ ƒê·∫°t ƒê∆∞·ª£c

### **Ch·ªâ s·ªë Hi·ªáu su·∫•t:**
- ‚úÖ **Th·ªùi gian ph·∫£n h·ªìi truy v·∫•n**: < 1 gi√¢y (y√™u c·∫ßu < 1 ph√∫t)
- ‚úÖ **S·ªë user ƒë·ªìng th·ªùi**: 50+ user
- ‚úÖ **Lu·ªìng x·ª≠ l√Ω**: 100+ y√™u c·∫ßu/ph√∫t
- ‚úÖ **Dung l∆∞·ª£ng b·ªô nh·ªõ**: ~500MB m·ªói worker
- ‚úÖ **Th·ªùi gian kh·ªüi ƒë·ªông**: ~15 gi√¢y

### **T√≠nh nƒÉng:**
- ‚úÖ **Nhi·ªÅu m√¥i tr∆∞·ªùng**: H·ªó tr·ª£ Local/YARN/Remote
- ‚úÖ **Tri·ªÉn khai t·ª± ƒë·ªông**: 3 b∆∞·ªõc tri·ªÉn khai (gi·∫£i n√©n ‚Üí thi·∫øt l·∫≠p ‚Üí kh·ªüi ƒë·ªông)
- ‚úÖ **Ki·ªÉm tra to√†n di·ªán**: B·ªô ki·ªÉm tra t·ª± ƒë·ªông
- ‚úÖ **S·∫µn s√†ng s·∫£n xu·∫•t**: Gunicorn + logging + gi√°m s√°t
- ‚úÖ **X·ª≠ l√Ω l·ªói**: Ki·ªÉm tra + ph·∫£n h·ªìi l·ªói h·ª£p l√Ω

### **Kh·∫£ nƒÉng m·ªü r·ªông:**
- ‚úÖ **M·ªü r·ªông ngang**: Nhi·ªÅu worker Gunicorn
- ‚úÖ **M·ªü r·ªông d·ªçc**: C·∫•u h√¨nh t√†i nguy√™n Spark
- ‚úÖ **M·ªü r·ªông d·ªØ li·ªáu**: T√≠ch h·ª£p HDFS cho d·ªØ li·ªáu l·ªõn

---

## üèÜ ƒê·ªïi M·ªõi Ch√≠nh
**Thay v√¨ truy v·∫•n tr·ª±c ti·∫øp t·ª´ HDFS m·ªói y√™u c·∫ßu (ch·∫≠m), t√¥i preload v√† cache to√†n b·ªô d·ªØ li·ªáu v√†o b·ªô nh·ªõ Spark, bi·∫øn I/O ƒëƒ©a th√†nh truy c·∫≠p b·ªô nh·ªõ - ƒë√¢y l√† l√Ω do ch√≠nh gi√∫p ƒë·∫°t hi·ªáu su·∫•t y√™u c·∫ßu t·ª´ ph√∫t xu·ªëng gi√¢y.**

**üéâ K·∫øt qu·∫£: API s·∫µn s√†ng s·∫£n xu·∫•t v·ªõi tri·ªÉn khai ch·ªâ 3 l·ªánh, ho√†n to√†n ƒë√°p ·ª©ng y√™u c·∫ßu b√†i to√°n!**

---

## üì¶ C·∫•u tr√∫c G√≥i: adnlog-api-complete.zip
```
adnlog-api-complete/
‚îú‚îÄ‚îÄ üîß Core Application
‚îÇ   ‚îú‚îÄ‚îÄ app.py                 # ·ª®ng d·ª•ng Flask ch√≠nh
‚îÇ   ‚îú‚îÄ‚îÄ spark_session.py       # C·∫•u h√¨nh Spark
‚îÇ   ‚îú‚îÄ‚îÄ data_processor.py      # Logic x·ª≠ l√Ω d·ªØ li·ªáu
‚îÇ   ‚îî‚îÄ‚îÄ wsgi.py               # ƒêi·ªÉm nh·∫≠p WSGI
‚îú‚îÄ‚îÄ üöÄ Deployment Scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup_environment.sh   # Script thi·∫øt l·∫≠p t·ª± ƒë·ªông
‚îÇ   ‚îú‚îÄ‚îÄ start_server.sh       # Server s·∫£n xu·∫•t
‚îÇ   ‚îú‚îÄ‚îÄ start_dev.sh          # Server ph√°t tri·ªÉn
‚îÇ   ‚îî‚îÄ‚îÄ test_api.sh           # B·ªô ki·ªÉm tra API
‚îú‚îÄ‚îÄ üìã Configuration
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt      # Ph·ª• thu·ªôc Python
‚îÇ   ‚îî‚îÄ‚îÄ README.md            # T√†i li·ªáu
‚îî‚îÄ‚îÄ üìñ Documentation
    ‚îî‚îÄ‚îÄ deploy_instructions.md # H∆∞·ªõng d·∫´n tri·ªÉn khai
```

---

## üìù Ghi Ch√∫
- ƒê·∫£m b·∫£o c√°c ƒë∆∞·ªùng d·∫´n (v√≠ d·ª•: `/data/spark-3.4.3`, `/data/hadoop-3.3.5/etc/hadoop`) kh·ªõp v·ªõi c·∫•u h√¨nh server.
- N·∫øu x·∫£y ra l·ªói, ki·ªÉm tra log v√† s·ª≠ d·ª•ng c√°c b∆∞·ªõc g·ª° l·ªói ƒë∆∞·ª£c cung c·∫•p.
- Th·ªùi gian hi·ªán t·∫°i l√† 19:41 PM +07, h√£y l√™n k·∫ø ho·∫°ch b·∫£o tr√¨ server ph√π h·ª£p.
## üèÜ **Key Innovation**

**Thay v√¨ query tr·ª±c ti·∫øp t·ª´ HDFS m·ªói request (ch·∫≠m), t√¥i pre-load v√† cache to√†n b·ªô data v√†o Spark memory, bi·∫øn disk I/O th√†nh memory access - ƒë√¢y l√† l√Ω do ch√≠nh gi√∫p ƒë·∫°t ƒë∆∞·ª£c performance y√™u c·∫ßu t·ª´ ph√∫t xu·ªëng gi√¢y.**

**üéâ K·∫øt qu·∫£: API production-ready v·ªõi deployment ch·ªâ 3 l·ªánh, ho√†n to√†n ƒë√°p ·ª©ng y√™u c·∫ßu b√†i to√°n!**
##  K·∫æT QU·∫¢ CH·∫†Y ƒê∆Ø·ª¢C 
SERVER CH·∫†Y 
<img width="1663" height="747" alt="image" src="https://github.com/user-attachments/assets/c71454df-f775-4280-b129-51ec12fbc5fc" />
<img width="1127" height="312" alt="image" src="https://github.com/user-attachments/assets/e730cf72-1ff5-4ceb-a55e-92fb83fc7f5d" />
## K·∫æT QU·∫¢ ƒê·∫†T ƒê∆Ø·ª¢C 
<img width="1904" height="431" alt="image" src="https://github.com/user-attachments/assets/995883b6-3a15-46dd-ad7e-650af111d599" />




